# -*- coding: utf-8 -*-
"""power_consumption_prod.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10XC9_E9IDltnLwLNrHSRcmUXMWcM9BSN
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_absolute_error,mean_squared_error
from sklearn.feature_selection import SelectKBest,mutual_info_regression


# Dataset uses ';' as a separator and contains missing values represented by '?' or blank spaces
df = pd.read_csv('household_power_consumption.txt', sep=';',na_values= [' ', '?'] , low_memory=False)


# lag features represent past values of the target variable. They help the model capture temporal dependency.
df['lag_1'] = df['Global_active_power'].shift(1)  # 1 step back
df['lag_2'] = df['Global_active_power'].shift(2)  # 2 steps back
df = df.dropna(subset=['lag_1', 'lag_2']) # dropping rather than using ffill and bfill to get false information
df['Global_active_power'] =  pd.to_numeric(df['Global_active_power'].replace(['?', ' '], np.nan), errors='coerce')
df = df.dropna(subset=['Global_active_power'])
df['Datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'], format='%d/%m/%Y %H:%M:%S')
df = df.sort_values('Datetime').reset_index(drop=True)
df['hour'] = df['Datetime'].dt.hour
df['day_of_week'] = df['Datetime'].dt.dayofweek
df['month'] = df['Datetime'].dt.month

num_cols = ['Global_reactive_power','Voltage','Global_intensity','Sub_metering_1'	,'Sub_metering_2','Sub_metering_3','lag_1','lag_2',	'hour','day_of_week','month']
for col in num_cols:
  df[col] = pd.to_numeric(df[col].replace(['?', ' '], np.nan), errors='coerce')

x = df.drop(columns=['Global_active_power', 'Date', 'Time']) # we have combined Datetime so dont need date time separately
x.isna().sum()
df.isna().sum()
# #combining Datetime help us with extracting[hours , weeks, ...] , Sorting, Splitting easier & less complicated.
y = df['Global_active_power']   # target

# #  # splitting the data by Datetime
split_date = '2010-01-01'

x_train = x[x['Datetime'] < split_date]
x_test  = x[x['Datetime'] >= split_date]

y_train = y[df['Datetime'] < split_date]
y_test  = y[df['Datetime'] >= split_date]

# # # x_train.loc[:, 'hour'] = x_train['Datetime'].dt.hour
# # # x_train.loc[:, 'day_of_week'] = x_train['Datetime'].dt.dayofweek
# # # x_train.loc[:, 'month'] = x_train['Datetime'].dt.month


# # # x_test.loc[:, 'hour'] = x_test['Datetime'].dt.hour
# # # x_test.loc[:, 'day_of_week'] = x_test['Datetime'].dt.dayofweek
# # # x_test.loc[:, 'month'] = x_test['Datetime'].dt.month


x_train = x_train.drop(columns=['Datetime'])
x_test = x_test.drop(columns=['Datetime'])

# # num_cols = ['Global_reactive_power','Voltage','Global_intensity','Sub_metering_1'	,'Sub_metering_2','Sub_metering_3','lag_1','lag_2',	'hour','day_of_week','month']



num_preprocess = Pipeline([
    ("imputer", SimpleImputer(strategy="median"))
])

preprocessor = ColumnTransformer([
    ("num", num_preprocess, num_cols)
])


pipeline = Pipeline([
    ('preprocess', preprocessor),
    ('selector', SelectKBest(mutual_info_regression, k=10)),
    ('classifier',   RandomForestRegressor(max_depth= 10, min_samples_leaf= 1, n_estimators= 20,random_state=42))
])

pipeline.fit(x_train, y_train)

y_pred = pipeline.predict(x_test)

print("Mean Square Error(MSE)",mean_squared_error(y_test, y_pred))
print("Root Mean Squared Error (RMSE)",np.sqrt(mean_squared_error(y_test, y_pred)))
print("Mean Absolute Error (MAE)" , mean_absolute_error(y_test, y_pred))
print("R² Score (Coefficient of Determination)",r2_score(y_test, y_pred))


# Mean Square Error(MSE) 0.0010836584181480387
# Root Mean Squared Error (RMSE) 0.032918967452641015
# Mean Absolute Error (MAE) 0.01776634578802409
# R² Score (Coefficient of Determination) 0.998755059914892

import joblib

# Saving the pipeline
joblib.dump(pipeline, 'power_consumption_model.pkl')
print("Pipeline saved as power_consumption_model.pkl")

